{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbuZfFWZVXpn"
   },
   "source": [
    "<h1><center>Cloud Computing  et informatique distribu√©e</center></h1>\n",
    "<h2>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Exercises: TP 2 on Map Reduce and Spark </center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBLI_qUSVXpq"
   },
   "source": [
    "# Objectives \n",
    "\n",
    "<ul>\n",
    "    <li>  Apache Parquet</li>\n",
    "    <li>  Pyspark </li> \n",
    "    <li>  Pandas library on Spark</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42q0jiKKVXpr"
   },
   "source": [
    "# Context\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For running this serie of exercises we are going to use a quite big dataset containing data on Bitcoin made available from <a href=\"https://www.kaggle.com/mczielinski/bitcoin-historical-data\">Kaggle</a>.\n",
    "\n",
    "As stated in the description of the dataset:\n",
    "\"Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary.\" \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "### The dataset\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The dataset is in a .csv file:\n",
    "\n",
    "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
    "\n",
    "CSV files for select bitcoin exchanges for the time period of Jan 2012 to December March 2021, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. \n",
    "\n",
    "Notice that:\n",
    "<ul>\n",
    "    <li> Timestamps are in Unix time.</li>\n",
    "<li> imestamps without any trades or activity have their data fields filled with NaNs. </li>\n",
    "<li>  TIf a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforeseen technical error in data reporting or gathering. </li>\n",
    "</ul>\n",
    "As stated by the authors \"all effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk\".\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd5y1A2BVXpr"
   },
   "source": [
    "# Environment set-up\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As first step you must include your dataset in your environment then you have to download the \n",
    "\n",
    "\n",
    "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
    "    \n",
    "and upload it in the folder where your notebook is supposed to read the input.\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As second step you must prepare your environment running the following two cells that:\n",
    "<ul>\n",
    "    <li> Import the Pandas library .</li>\n",
    "<li> Set the Spark environment and return a SparkSession (acting as was acting the SparkContext in the previous exercises). </li>\n",
    "</ul>    \n",
    "    \n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVZaAkIgVXps"
   },
   "outputs": [],
   "source": [
    "# import of Pandas library\n",
    "import pandas as pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnIAJZrfVXps",
    "outputId": "7a5bf029-d532-4186-b1d2-f8c146f7e7e9"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import of the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#inizialization of the Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TP2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOcFr4uSV03u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdYKtVF_VXpt"
   },
   "source": [
    "##  File import\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise the goal is to create a Spark DataFrame from the csv file in imput. \n",
    "\n",
    "Recall that in Spark DataFrame the type of the columns is very important for the definition of the internal data representation. \n",
    "    \n",
    "For this step you the target set of typed columns is the following one: \n",
    "<ul>\n",
    "    <li>    $Date\\_Time: Timestamp$ </li>\n",
    "     <li>   $Open: double$ </li>\n",
    "     <li>   $High: double$ </li>\n",
    "    <li>    $Low: double$ </li>\n",
    "    <li>    $Close: double$ </li>\n",
    "    <li>    $Volume\\_BTC: double$ </li>\n",
    "    <li>    $Volume\\_Currency: double$ </li>\n",
    "    <li>    $Weighted\\_Price: double$ </li>\n",
    "</ul>\n",
    "    \n",
    "We will arrive to define the schema in 3 steps.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Notice that the header of the $csv$ file contains the data description and that the simple import of the\n",
    "file treats the timestamp column as a String. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In data import you must check that:\n",
    "<ul>\n",
    "    <li>  the types of the imported data (the ones read from the file using the operation you choose) are equal to the types in the given schema</li>\n",
    "    <li>  the names of columns correspond (and make transofrmations if necessary) </li> \n",
    "</ul>\n",
    "    \n",
    "The header that can be imported and used for the definition of the names of the columns\n",
    "<ul>\n",
    "    <li>  the types of the import data are equal to the types in the given schema</li>\n",
    "    <li>  the name of columns correspond (and make transofrmations if necessary </li> \n",
    "</ul>\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQZN4xiqVXpt"
   },
   "source": [
    "## 1. First import\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Import the csv file in Spark DataFrame. If you have any doubt you can always refer to the Spark 3.1.1 documentation:\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/3.1.1/\">Kaggle</a>\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imMeOHQmVXpu",
    "outputId": "eea247cd-df1f-4d44-a1f6-07431714b524"
   },
   "outputs": [],
   "source": [
    "#Write the command that creates a Spark DataFrame and stores the reference in the dfs variable\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "dfs =\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfs\n",
    "\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double]</font>\n",
    "#\n",
    "# Notice that if you have something like:\n",
    "# DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]\n",
    "# you forgot a step\n",
    "#\n",
    "# Notice also that if you have:\n",
    "# DataFrame[Timestamp: string, Open: string, High: string, Low: string, Close: string, Volume_(BTC): string, Volume_(Currency): string, Weighted_Price: string]\n",
    "# you also forgot a step\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTbhA1Aip4mM"
   },
   "outputs": [],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkQ8aYX-VXpv"
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "You notice that the import has two problems with respect to the target schema:\n",
    "<ul>\n",
    "    <li>  the $Date\\_Time$ column is not present in the original file and  there is an $int$ column $Timestamp$ that can be converted to Date</li> \n",
    "    <li> some of the column names contain not required parentesis </li>\n",
    "</ul>     \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUS9JWIcVXpv"
   },
   "source": [
    "## 2.Timestamp column \n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Refine the import of the csv file and convert the \"timestamp\" column in the proper $Timestamp$ type:\n",
    "    <ul>\n",
    "     <li>   Create a new column $Date_Time$ that is the conversion of the $String$ column $Timestamp$ in $Timestamp$ type  </li>\n",
    "</ul>\n",
    "The Dataframe are immutable structure, then your procedure will use a command (discussed in the slides) that will create a new Spark $DataFrame$ from the $dfs$ $DataFrame$ having a different schema. \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the timestamp column of the csv file and from the imported DataFrame \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pj8BeALtVXpv",
    "outputId": "558fe50d-ee94-429c-d0d3-f98b8bd76dfa"
   },
   "outputs": [],
   "source": [
    "# write the command that creates a new Data Frame Spark with Date_Time column\n",
    "# and store the reference in the dfsdt variable (Data Frame Spark with Date_Time)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "dfsdt = \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfsdt\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
    "#Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double, Date_Time: timestamp]\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBcoKytNVXpw",
    "outputId": "0f2853a9-85dc-4240-e978-4b999fba9ba8"
   },
   "outputs": [],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfsdt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWEJTFojVXpw"
   },
   "source": [
    "## 3. Column names\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As you can see from the output of the previous exercise the names of the columns still present some problems since there are some parentesis that are not required.\n",
    "    <ul>\n",
    "     <li> Remove the not required parentesis from the colum names </li>\n",
    "     <li> Hint: look at the documentation of DataFrame API and check the operation for column renaming </li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRv_VbgYVXpw",
    "outputId": "ef9dac39-4e8c-47a4-e5c0-b9416c4d6167"
   },
   "outputs": [],
   "source": [
    "# write the command that creates a new Data Frame Spark with correct names for all the columns\n",
    "# and store the reference in the dfscr variable (Data Frame Spark with Correct Names)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "dfscr = \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfscr\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "#DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
    "#          Volume_BTC: double, Volume_Currency: double, Weighted_Price: double]\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YU2SFo0UVXpw",
    "outputId": "370db461-16ce-467f-c9ab-c7d567a03e5c"
   },
   "outputs": [],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfscr.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
    "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
    "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|\n",
    "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|\n",
    "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|\n",
    "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|\n",
    "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw7ktEJKVXpx"
   },
   "source": [
    "## DataFrame columns \n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "In this part of the exercise we are going continue to  modify in the Spark DataFrames.\n",
    "\n",
    "    \n",
    "Remember that using  PySpark, it's possible to access a DataFrame's columns either by attribute ($df.attributeName$) or by indexing $(df['attributeName'])$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "Loook at the list of the functions to get familiar with the documentation: some functions that can be of help to manipulate the schema:\n",
    "    \n",
    "<ul>\n",
    "     <li>    <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>.  </li>\n",
    "</ul>    \n",
    "    \n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBgUaY_oVXpx"
   },
   "source": [
    "## 4.  Add two new columns to the DataFrame\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to extend the DataFrame with two other columns: given the $Date\\_Time$ column create two new columns ($Year$ and $Month$) that contain \n",
    "    <ul>\n",
    "     <li> the year </li>\n",
    "     <li> the month of the year </li>\n",
    "</ul>\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDH7YsSeVXpx"
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">    \n",
    "Look at the documentation of Spark functions and find the two functions that are convenient for this use case (hint: the name of the columns can help: <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>)\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpL3519cVXpx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the functions that you will use\n",
    "\n",
    "############## WRITE YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "\n",
    "############## END OF THE EXERCISE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsehHsySVXpz",
    "outputId": "734b7b8b-a0ca-4777-fe4f-dd013113fe02"
   },
   "outputs": [],
   "source": [
    "# write the command that creates a new Data Frame Spark with the two additional columns\n",
    "# and store the reference in the dfsym variable (Data Frame Spark with Correct Names)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "dfsym = \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "dfsym.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
    "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
    "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
    "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
    "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDMJbhTbVXpz"
   },
   "source": [
    "###  5.  Drop Timestamp\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Finally we clean the schema and we can remove the the $Timestamp$ column.\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qm8I1zhzVXp0",
    "outputId": "9de536e8-8216-47d3-87fb-4eda9f990371"
   },
   "outputs": [],
   "source": [
    "# write the command that creates a new DataFrame Spark from the dfsym without the Timestamp column\n",
    "# and store the reference in the dfc variable (Data Frame Spark Clean)\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "dfsc = \n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "dfsc.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dm-ovIcVXp0"
   },
   "source": [
    "###  6 Save data in Parquet\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In order to gain in performance in the following it is a good idea, as we have seen at lesson, to use Parquet that will\n",
    "    allow \n",
    "to partition the SparkDataframe and to store it in multiple Parquet files. \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "For this first example partition the file according to:\n",
    "    \n",
    " <ul>\n",
    "     <li> the year </li>\n",
    "             <li> the month of the year </li>\n",
    "</ul>\n",
    "The $partitionBy()$ operation can help for this step (Documentation of reference: <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\">Spark Functions</a>).\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84UNDFHiVXp0",
    "outputId": "6ca3128d-15f6-45ec-e3fd-a2d24a4cca92"
   },
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTR8AxS2VXp0",
    "outputId": "f10d3f3d-31e3-4f26-cc2f-92c8c3aa4ee0"
   },
   "outputs": [],
   "source": [
    "# write the command that saves the dfsc DataFrame in Parquet\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "print(\"write to Parquet done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic-eLcr0VXp0"
   },
   "source": [
    "###  7 Folder Structure\n",
    "\n",
    " \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the folder structure that has been created for the storage of the file and describe it.\n",
    "\n",
    "While you navigate (and the folder structure) data remember that in the data access:\n",
    "    \n",
    " <ul>\n",
    "     <li> the navigation is done using Parquet </li>\n",
    "     <li> the leaf contain the encoded Parquet files </li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dC1C75uIVXp1",
    "outputId": "4d8659e1-1963-40bb-c773-89d7cd965c26"
   },
   "outputs": [],
   "source": [
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "#BTC\n",
    "#        ‚îú‚îÄ‚îÄ Year=2011\n",
    "#        ‚îÇ   ‚îú‚îÄ‚îÄ ...\n",
    "#        ‚îÇ   ‚îÇ\n",
    "#        ‚îÇ   ‚îú‚îÄ‚îÄ month=12\n",
    "#        ‚îú‚îÄ‚îÄ Year=2012\n",
    "#        ‚îÇ   ‚îú‚îÄ‚îÄ month=1\n",
    "#        ‚îÇ   ‚îú‚îÄ‚îÄ ...\n",
    "#        ‚îÇ   ‚îÇ\n",
    "#       ...\n",
    "        \n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMCbaNcPVXp1"
   },
   "source": [
    "###  8 Pandas\n",
    "\n",
    " \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "This data organization opens the opportunity to read data also using Pandas and not using Parquet.\n",
    "    \n",
    "Look at the documentation and check how you can read a Parquet structure and store it in a Pandas DataFrame:\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html\">Pandas and Parquet</a>\n",
    "\n",
    "Notice how at the data-exchange base there is the presence of Arrow (thanks to $pyarrow$).\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Write the command that using Pandas read the data for the year 2011.\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B11RsK9YVXp1"
   },
   "outputs": [],
   "source": [
    "#import of pandas\n",
    "import pandas as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "jlRYuW0hVXp1",
    "outputId": "6f760452-1bda-44eb-b12f-8a31b25a15de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame using Pandas functions and reading the data only for the year 2011/\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "df = \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "df\n",
    "#######################\n",
    "# Expected output:\n",
    "#Open\tHigh\tLow\tClose\tVolume_BTC\tVolume_Currency\tWeighted_Price\tDate_Time\tMonth\n",
    "#0\t4.39\t4.39\t4.39\t4.39\t0.455581\t2.0\t4.39\t2011-12-31 07:52:00\t12\n",
    "#1\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:53:00\t12\n",
    "#2\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:54:00\t12\n",
    "#3\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:55:00\t12\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAYamKaEVXp1"
   },
   "source": [
    "###  8a Read Parquet file\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Load now the the spark DataFrame from Parquet\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6C0iA8SqVXp2",
    "outputId": "351937bf-257f-4259-97e5-7f5090241429"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame using spark and reading the whole data/\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "dfs = \n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "print(\"read done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjH0cqC5VXp2"
   },
   "source": [
    "###  9 Verify number of column and count the number of rows\n",
    "    \n",
    "<p align=\"justify\">\n",
    "Maybe you have not noticed that the volume of data we are treating is not so small as it seems. \n",
    "Count how many rows we are manipulating in the dataframe $dfs$\n",
    "<font size=\"3\">\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYqK9VD8VXp2",
    "outputId": "9adb3ef8-6140-4e79-dd3b-fe9b0aaf91a5"
   },
   "outputs": [],
   "source": [
    "# Write the command that returns the number of rows of the DataFrame\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "count = \n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "print(count)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "# 4857377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJVbEunCVXp2",
    "outputId": "ec816bb9-c8e1-48c0-838f-04868b8eb367"
   },
   "outputs": [],
   "source": [
    "#We can also check and verify the schema of the DataFrame\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoAfjdAuVXp3"
   },
   "source": [
    "###  Statistics\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin by month for all the years.\n",
    "\n",
    "The computed statistics will be stored in a DataFrame having this schema\n",
    "<ul>\n",
    "     <li>   Mean_Vol  : double </li>\n",
    "     <li>   Std_Vol   : double </li>\n",
    "     <li>   Min_Vol   : double </li>\n",
    "     <li>   Max_vol   : double </li>\n",
    "     <li>   Year      : int </li>\n",
    "     <li>   Month     : int </li>\n",
    "  \n",
    "</ul>\n",
    "\n",
    "In this exercise you will have two develop different methodologies to compute the statistics:\n",
    "<ul>\n",
    "     <li>   using the $applyInPandas()$ Pyspark function and the Pandas functions </li>\n",
    "     <li>   only using the Pyspark functionnalities </li>\n",
    "</ul>\n",
    "The statistics computed should be stored in a Pandas DataFrame with both the two approaches.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gts_hy5HVXp4"
   },
   "source": [
    "### 10 Spark applyinPandas\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The solution with $applyinPandas$ \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icjKlPHcVXp4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# the Python function that must be used. \n",
    "\n",
    "def compute_stats(key,df):\n",
    "    res = df[\"Volume_BTC\"].describe()\n",
    "\n",
    "    res_dict = {}\n",
    "    for index, value in res.items():\n",
    "\n",
    "        if index == \"mean\":\n",
    "            res_dict[\"Mean_Vol\"] = value\n",
    "        elif index == \"std\":\n",
    "            res_dict[\"Std_Vol\"] = value\n",
    "        elif index == \"min\":\n",
    "            res_dict[\"Min_Vol\"] = value\n",
    "        elif index == \"max\":\n",
    "            res_dict[\"Max_Vol\"] = value\n",
    "\n",
    "    final =  pa.DataFrame([res_dict])\n",
    "    final[\"Year\"]  = key[0]\n",
    "    final[\"Month\"] = key[1]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsqKcxxNVXp4"
   },
   "source": [
    "### 11 The two parameters of the Python function\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the documentation of the $applyinPandas$ <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html\">applyinpandas</a> and describe how it works in detail from the DataFrame point of view in our example (what the $key$ and the $df$ will contain in our example).\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99_WnUHTVXp5"
   },
   "source": [
    "#### WRITE YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shVh5lBcVXp5"
   },
   "source": [
    "### 11 The two parameters in action\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Compute the statistics using then the $applyInPandas$ and the provided functions. \n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IebDul7NVXp6",
    "outputId": "14283103-717a-46ac-e77b-90c8606b29d3"
   },
   "outputs": [],
   "source": [
    "schema = \"Mean_Vol double, Std_Vol double, Min_Vol double, Max_Vol double, Year int, Month int\"\n",
    "\n",
    "# Write the command that will store in the variable statsdf the DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "statsdf = \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "statsdf.show(5)\n",
    "\n",
    "\n",
    "####### EXPECTED OUTPUT\n",
    "#+------------------+------------------+----------+------------+----+-----+\n",
    "#|          Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
    "#+------------------+------------------+----------+------------+----+-----+\n",
    "#| 20.39613620802532| 54.24699556644988|    9.4E-5|2258.8231405|2012|   10|\n",
    "#|12.095179597807542|44.149334198665166| 2.0452E-4|2037.2239038|2015|    2|\n",
    "#| 6.147061206279663|17.745599117954125|0.00127783|564.21436237|2019|   10|\n",
    "#| 8.468866447160776|  28.9837002907642|    1.0E-8|1616.0600006|2017|    3|\n",
    "#| 8.684880075589284| 17.69646210434965|       0.0|533.10078293|2017|    8|\n",
    "#+------------------+------------------+----------+------------+----+-----+#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Pdf1N-VXp6"
   },
   "source": [
    "### 12 the statsdf DataFrame\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Which kind of DataFrame is statsdf?\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbXKRuBEVXp6"
   },
   "source": [
    "#### WRITE YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgGmWPhSVXp7"
   },
   "source": [
    "\n",
    "### 13 DataFrame in Pandas\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Since we computed a stat by month the results will be small (we will have only one row by month)\n",
    "we can get and handle all the results in memory in Pandas.  \n",
    "    \n",
    "Notice that Spark is lazy so the $toPandas$ action will trigger the computation.\n",
    "    \n",
    "Write the command that will do the operation.\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wupi4CTgVXp7",
    "outputId": "68b9c3a7-828b-4c79-85a1-9e1a21e6b197"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Write the command that will store in the variable stats_dfp the outoput DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "stats_dfp = \n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "Ve6u_nzSVXp8",
    "outputId": "33e4f202-1b81-4b81-d992-fb180fd96a31"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "stats_dfp.head(10)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#\tMean_Vol\tStd_Vol\tMin_Vol\tMax_Vol\tYear\tMonth\n",
    "#0\t20.396136\t54.246996\t9.400000e-05\t2258.823141\t2012\t10\n",
    "#1\t12.095180\t44.149334\t2.045200e-04\t2037.223904\t2015\t2\n",
    "#2\t6.147061\t17.745599\t1.277830e-03\t564.214362\t2019\t10\n",
    "#3\t8.468866\t28.983700\t1.000000e-08\t1616.060001\t2017\t3\n",
    "#4\t8.684880\t17.696462\t0.000000e+00\t533.100783\t2017\t8\n",
    "#5\t16.040933\t57.641501\t2.044000e-05\t4111.876106\t2014\t4\n",
    "#6\t4.984386\t18.903445\t1.054000e-05\t822.866974\t2020\t6\n",
    "#7\t8.331579\t18.350084\t5.758000e-04\t806.636224\t2019\t5\n",
    "#8\t8.621910\t18.820399\t4.047000e-04\t602.282607\t2017\t10\n",
    "#9\t3.106413\t10.738051\t3.300000e-06\t582.564185\t2018\t10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX4X2ioqVXp8"
   },
   "source": [
    "###  14 Show the stats of the stats\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin by month for all the years.\n",
    "\n",
    "The computed statistics will be stored in a DataFrame having this schema\n",
    "<ul>\n",
    "     <li>   the min of the set min values </li>\n",
    "     <li>   the mean of the set of mean values </li>\n",
    "     <li>   ... </li> \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FFQDxOyVXp8",
    "outputId": "727269bf-87c6-4d02-88d7-b0815ad571fa"
   },
   "outputs": [],
   "source": [
    "# Write the command that will show and compute the stats on the numerical columns of the statsdf DataFrame\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
    "#|summary|          Mean_Vol|           Std_Vol|             Min_Vol|           Max_Vol|              Year|             Month|\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
    "#|  count|               112|               112|                 112|               112|               112|               112|\n",
    "#|   mean|10.782191354847754|28.871463944232485|0.004551177678571...|1067.2847720235718|2016.0892857142858| 6.428571428571429|\n",
    "#| stddev| 6.488551661205522| 18.11344145463867|0.043048607639448476| 895.8083462469303|2.7164947320662614|3.5252353718985097|\n",
    "#|    min| 2.929999689326444| 6.490701567379118|                 0.0|       43.31219578|              2011|                 1|\n",
    "#|    max|31.504423573146152|106.97606692383131|          0.45558087|      5853.8521659|              2021|                12|\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wtYR3CaVXp8"
   },
   "source": [
    "### Plotting \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to plot the resutls of the statistics by year and month (that will be in the $x$ orizontal axis of the plot). \n",
    "\n",
    "$Plotly$ will be used for the plotting\n",
    "    \n",
    "\n",
    "This provided version of the code is fully working in Python.\n",
    "    \n",
    "\n",
    "A Python routine converts the two columns $Year$ and $Month$ into a $DateTime$ column 'Date' (in order to plot the data in relation with the date).\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "5E1eQmpJVXp9",
    "outputId": "b878a08e-b1eb-4feb-c1a9-1414f2407b79"
   },
   "outputs": [],
   "source": [
    "#install plotly and import the libraries\n",
    "\n",
    "!pip install plotly\n",
    "\n",
    "from plotly.offline import iplot,init_notebook_mode\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlyn4HLZVXp9"
   },
   "outputs": [],
   "source": [
    "#Helper function that converts the Year Month of our data into Date type\n",
    "\n",
    "def get_date_from_year_month(df):\n",
    "    df[\"Date\"] = pa.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
    "    return df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "mS9hH6hHVXp9",
    "outputId": "afea0a7c-4d28-4790-f4ff-8eaa866e479d"
   },
   "outputs": [],
   "source": [
    "# In this phase we need to sort by the date to allow parallelisation of shuffled the results\n",
    "\n",
    "stats_dfp = get_date_from_year_month(stats_dfp)    \n",
    "stats_dfp.sort_values(by = 'Date',inplace = True)\n",
    "stats_dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UunBZ10kVXp-",
    "outputId": "eb28cb0a-c8d9-4725-bb5f-687cf66c94bc"
   },
   "outputs": [],
   "source": [
    "# PLOTTING OF THE MEAN VOLUME BY MONTH\n",
    "mean_vol_trac = {\n",
    "    \"x\": stats_dfp.Date,\n",
    "    \"y\": stats_dfp[\"Mean_Vol\"],\n",
    "}\n",
    "\n",
    "layout = {\n",
    "  \"height\":1000,\n",
    "  \"showlegend\": True, \n",
    "  \"title\": \"Average Volume by Month of BTC\",\n",
    "}\n",
    "\n",
    "fig = go.Figure(data=[mean_vol_trac], layout=layout)\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1XHlwrvVXp-"
   },
   "source": [
    "###  15 Compute the statistics using Pyspark\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin as we did before but using Pandas.\n",
    "\n",
    "The steps will be:\n",
    "<ul>\n",
    "     <li>   import data from Parquet in a Spark DataFrame </li>\n",
    "     <li>   remove null values </li>\n",
    "     <li>   perform the aggregation of the results </li> \n",
    "     <li>   convert the results to Pandas </li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcS4_jawVXp_",
    "outputId": "8cac138c-357d-4ab2-ade7-ca468f069fd5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# solution to compute the statistics using pyspark function \n",
    "\n",
    "from pyspark.sql.functions import min, max, mean, stddev\n",
    "\n",
    "\n",
    "# full spark dataframe (recall exercise 8a)\n",
    "df_spark = spark.read.parquet(\"BTC/\") \n",
    "\n",
    "# the na drop is important to be able to compute properly the stats\n",
    "# look at the documentation of the na.drop function\n",
    "group_ym = df_spark.na.drop().select([\"Volume_BTC\",\"Year\",\"Month\"]).groupBy([\"Year\",\"Month\"])\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "           \n",
    "    \n",
    "# aggregation \n",
    "# notice that the argument of the agg function is strictly related to min Vol, max Vol, mean, and stddev.\n",
    "res_df = \n",
    "\n",
    "\n",
    "#conversion the results to pandas\n",
    "stats_dfs = \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "stats_dfs = get_date_from_year_month(stats_dfs)    \n",
    "stats_dfs.sort_values(by = 'Date',inplace = True)\n",
    "stats_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAzvUAQJVXp_"
   },
   "source": [
    "### 16 Equivalence of results\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now that you have seen the two procedures to get the results you must compare the outputs:\n",
    "<ul>\n",
    "     <li>   verify if the pandas dataframe from applyInPandas and PySpark functions are equivalents (look at the documentation to find the function that asserts if two DataFrames are equals) </li> \n",
    "         <li> compare the processing time between applyInPandas and PySpark routine with functions (that we have visualised with the %%time function) and comment them.</li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lngxwISVXp_"
   },
   "outputs": [],
   "source": [
    "# verify if the pandas dataframe from applyInPands and PySpark functions are equivalents \n",
    "# compare the processing time between applyInPandas and PySpark function\n",
    "cols = ['Mean_Vol', 'Std_Vol', 'Min_Vol', 'Max_Vol', 'Year', 'Month', 'Date']\n",
    " \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "# equivalence verification: look at the pandas API and check if there is any test/assertion operation of help\n",
    "\n",
    "\n",
    "# comment about time execution and draw your considerations\n",
    "#'''############## END OF THE EXERCISE ##############'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSw_cXcmVXqA"
   },
   "source": [
    "### 16 Plotting the financial data\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now that you have seen some examples you can draw your graphs:\n",
    "<ul>\n",
    "     <li>   filter the global data frame fron Parquet and take only the first day of the year 2021 </li> \n",
    "         <li> convert it to a pandas dataframe </li> \n",
    "         <li>    display the data using the $plot_candlestick$ routine </li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R05figbfVXqA"
   },
   "outputs": [],
   "source": [
    "#this function helps you to display the candlestick ( representation of financial data) of the pandas dataframe\n",
    "\n",
    "def plot_candlestick(df):\n",
    "    trace = {\n",
    "      \"x\": df.Date_Time,\n",
    "      \"close\": dfp[\"Open\"],\n",
    "      \"decreasing\": {\"line\": {\"color\": \"#008000\"}}, \n",
    "      \"high\":df[\"High\"] ,\n",
    "      \"increasing\": {\"line\": {\"color\": \"#db4052\"}}, \n",
    "      \"low\": df[\"Low\"],\n",
    "      \"name\": \"BTC\", \n",
    "      \"open\": df[\"Close\"],\n",
    "      \"type\": \"candlestick\"\n",
    "    }\n",
    "\n",
    "    layout = {\n",
    "      \"height\":1000,\n",
    "      \"showlegend\": True, \n",
    "      \"title\": \"Technical Analysis\",\n",
    "    }\n",
    "    \n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hu81JTxBVXqA",
    "outputId": "ce14bc33-ebdb-4110-a814-5c941c89f45c"
   },
   "outputs": [],
   "source": [
    "# Exercise filter the spark dataframe by date \n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "#read the global dataframe (as usual)\n",
    "df_spark = \n",
    "\n",
    "#create the beginning end date \n",
    "beg = \n",
    "end = \n",
    "\n",
    "\n",
    "#create the filter\n",
    "df_spark_filtered = \n",
    "\n",
    "#apply and convert it to pandas\n",
    "dfp = \n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5VQE8mhPVXqB",
    "outputId": "47d77723-8df8-47f2-841f-6d345ca33222",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_candlestick(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxGOfykpVXqB"
   },
   "source": [
    "### 17 Propose your analysis\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Think about a new analysis on this set of data to run on your data and run it showing a graph\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ewmVzLeuVXqB",
    "outputId": "c635b866-60d4-479f-b54c-221c0e78b681"
   },
   "outputs": [],
   "source": [
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "524h_jgeVXqB"
   },
   "source": [
    "###¬†Conclusion \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "$ApplyinPandas$ can be very powerful when you need to apply advanced Python code or Python libraries (i.e. <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>  otherwise you can use Pyspark routines relying on most powerful storage techniques for example using Parquet.\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG3YxgAzVXqB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kaggle_data_2021_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
